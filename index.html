<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-GDXSC5Y2BD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GDXSC5Y2BD');
</script>

<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<script src="./files/head.js"></script>

<meta name="viewport" content="width=device-width, initial-scale=1">

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<meta name="keywords" content="Berkeley,Deep,Reinforcement,Learning,Computer Science,Machine,Artificial,Intelligence">

<article class="post-content">
  <meta name="twitter:title" content="Offline Reinforcement Learning as One Big Sequence Modeling Problem"/>
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:image" content="https://people.eecs.berkeley.edu/~janner/trajectory-transformer/images/humanoid_padded.png" />
<article class="post-content">

<title>Quantifying Assistive Robustness Via the Natural-Adversarial Frontier</title>
<link rel="stylesheet" href="./files/font.css">
<link rel="stylesheet" href="./files/main.css">

<link rel="stylesheet" type="text/css"
    href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<style>
body {
  font-family: "Computer Modern Serif", serif;
  font-size: 14pt;
}


* {padding:0;margin:0;box-sizing:border-box;}
#video {
  position: relative;
  padding-bottom: 45%; /* 16:9 */
  height: 0;
}
#video iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 80%;
  height: 100%;
  transform: translateX(12.5%);
}

</style>

  <style type="text/css">/**
 * Style sheet used by new LibX tooltip code
 */

/* We insert a <div> with libx-tooltip style under the body.
 * This will inherit body's style - we can't afford to inherit undesirable
 * styles and we must redefine what we need.  OTOH, some things, e.g.
 * font-size, might be ok to be inherited to stay within the page's tone.
 */
.libx-tooltip {
    display: none;
    overflow: visible;
    padding: 5px;
    z-index: 100;
    background-color: #eee;
    color: #000;
    font-weight: normal;
    font-style: normal;
    text-align: left;
    border: 2px solid #666;
    border-radius: 5px;
    -webkit-border-radius: 5px;
    -moz-border-radius: 5px;
}

.libx-tooltip p {
    /* override default 1em margin to keep paragraphs inside a tooltip closer together. */
    margin: .2em;
}
</style><style type="text/css">/**
 * Style sheet used by LibX autolinking code
 */
.libx-autolink {
}

</style>

</head>

  <body>

    <div class="outercontainer">
      <div class="container">

        <div class="content project_title">
          <br>
          <h2>Quantifying Assistive Robustness Via the Natural-Adversarial Frontier</h2>
          <div class="authors">
            <a href="https://scholar.google.com/citations?user=lp8MBNwAAAAJ&hl=en">Jerry Zhi-Yang He</a>,
            <a href="https://zackory.com/">Zackory Erickson</a>,
            <a href="https://www.cs.utah.edu/~dsbrown/">Daniel S. Brown</a>, and
            <a href="http://people.eecs.berkeley.edu/~anca/">Anca D. Dragan</a>
          </div>
          <div>
            <span class="venue"><a href="https://corl2023.org/">CORL 2023</a></span>
            <span class="tag">
              <a href="">Paper</a>&nbsp;
              <a href="https://github.com/hzyjerry/ood-human">Code</a>&nbsp;
              <!-- <a href="https://bair.berkeley.edu/blog/2021/11/19/trajectory-transformer/">Blog</a>&nbsp; -->
              <a href="files/bib.txt">BibTex</a>&nbsp;
            </span>
          </div>
        </div>

        <br>

        <center>
        <b><font size="4">RIGID - Robustness via Generative Natural-Adversarial Optimization</font></b>
        <br>
        <video width=100% height=auto autoplay playsinline muted loop>
          <source src="images/video_pull.mp4" type="video/mp4">
        </video>
        <i>We can derive both natural human motions and adversarial human motions. Interestingly, when we jointly optimize for the two objectives, we arrive at human motions that are seemingly natural, yet cause catastrophic robot failures.</i>
        </center>
        <p>
        <br>

        <div class="content">
          <div class="text">
            <p>
              <div class="title"><b>Motivations</b></div>
              Our ultimate goal is to build robust policies for robots that assist people. What makes this hard is that people can behave unexpectedly at test time, potentially interacting with the robot outside its training distribution and leading to failures. Let's consider assistive robots, the pinnacle of human-robot interaction. You trained an RL policy to assist someone with disability in an activity of daily living, i.e. the person has an itch on their arm and they can't scratch themselves. How would you know that this policy is safe to deploy?
            </p>
          </div>

          <center>
            <video width=70% height=auto autoplay playsinline muted loop>
              <source src="images/intro_assist.mp4" type="video/mp4">
            </video>
          </center>

          <div>
            <div>
            <p>
            </p>
            </div>
          </div>

          <div class="text">
            <div>
            <p>As we know, learning-based policies work well in distribution, and are prone to adversarial attacks outside this distribution. One way to approach this is to generate adversarial human motions.
            But do they actually reveal of the policy's brittleness? In this case, the adversarial human learns to "hides" their arm so the robot can't reach the itch spot and fails. Since this is improbable to happen to normal users, and preventable with safety measures, the failure is acceptable and not really showing policy's brittleness
            </p>
            </div>
          </div>

          <center>
            <video width=70% height=auto autoplay playsinline muted loop>
              <source src="images/intro_adv.mp4" type="video/mp4">
            </video>
          </center>


          <div class="text">
            <div>
            <p>What we need is not just adversarial human motion, but adversarial and "natural". When optimizing the human motion this way, we find a much more concerning example: a small jerk in the arm could send the robot flailing and spinning around wildly. Since this may well happen in daily interactions, it is catastrophic and needs to be addressed.
            </p>
            </div>
          </div>


          <center>
            <video width=70% height=auto autoplay playsinline muted loop>
              <source src="images/intro_natural.mp4" type="video/mp4">
            </video>
          </center>

          <br>
          <br>

          <div>
            <div class="text">
              <div>
              <div class="title"><b>Computing the the Natural-Adversarial Frontier with RIGID</b></div>
              <p>But how natural/unnatural do we allow the motions to be? We propose looking at the entire natural-adversarial pareto frontier, which represents the robot's performance under all possible types of human motions. Our method called RIGID presents a way to efficiently compute this pareto frontier.<br>
              </p>
              </div>
            </div>
          </div>


          <center>
            <video width=70% height=auto autoplay playsinline muted loop>
              <source src="images/intro_frontier.mp4" type="video/mp4">
            </video>
          </center>

          <div class="text">
            <div>
            <p>Setting $\lambda \rightarrow \infty$ results in a policy $\tilde{\pi}_H$ that closely resembles $\pi_H$ and yields a high reward. On the other hand, setting $\lambda = 0$ leads to a policy $\tilde{\pi}_H$ that is purely adversarial and causes harm to the assistive task by inverting the environment reward. By selecting $\lambda \in [0, \infty)$, we arrive at a spectrum of human motions that interpolate between adversarial and natural. This is meaningful in Human-Robot Interaction because while we may not need to worry about purely adversarial human behaviour, as they are less likely to happen in reality, we need to take precaution against human behaviour that appears natural, yet causes robot failures.
            </p>
            </div>
          </div>
        </div>


        <br>
        <br>
        <div class="content">
          <div class="text">
            <p>
              <div class="title"><b>Generating Edge-Cases for Assistive Robot Policies</b></div>
              Turns out RIGID is quite powerful! We validates with user studies that the RIGID-generated frontier can indeed identify natural failure cases.
              Compared with a trained expert user, RIGID generates failure cases in an automatic and more systematic manner. In comparison, the expert user can fail the robot, but the resulting failure cases tend to be exaggerated and unnatural.
          </p>
          <center>
            <video width=70% height=auto autoplay playsinline muted loop>
              <source src="images/edge_vs_human.mp4" type="video/mp4">
            </video>
            &nbsp;&nbsp;&nbsp;&nbsp;
          </center>
          <div class="text">
            <p>
              <div class="title">Here we visualize human motions discovered on and below the frontier. Note that the natural failure cases on concentrated on the top middle part of the curve.
            </p>
          </div>
          <center>
            <video width=100% height=auto autoplay playsinline muted loop>
              <source src="images/edge_full.mp4" type="video/mp4">
            </video>
            <br>
          </center>
          <br>
          </div>
        </div>

        <div class="content">
          <div class="text">
            <p>
              <div class="title"><b>Measuring Robustness</b></div>
              Turns out Natural Adversarial Frontier is actually more powerful than generating edge cases. We can use its shape as a way to characterize the "robustness" of robot policies. Overall, the smaller the area-under-curve is, the safer the robot policy is against all naturally adversarial scenarios. In the paper, we use the Natural Adversarial Frontier to evaluate  SOTA methods in robustifying RL policies and find interesting results.  <br>
              <center>
                <img width=80% src="images/robust_measure.png">
              </center>
          </p>
          </div>
        </div>

        <br>
        <br>
        <div class="content">
          <div class="text">
              <div class="title"><b>Quantifying Assistive Robustness Via the Natural-Adversarial Frontier</b></div>
                 <div class="authors">
                  <a href="https://scholar.google.com/citations?user=lp8MBNwAAAAJ&hl=en">Jerry Zhi-Yang He</a>,
                  <a href="https://zackory.com/">Zackory Erickson</a>,
                  <a href="https://www.cs.utah.edu/~dsbrown/">Daniel S. Brown</a>, and
                  <a href="http://people.eecs.berkeley.edu/~anca/">Anca D. Dragan</a>
                </div>
                  <span class="venue"><a href="https://corl2023.org/">CORL 2023</a></span>
                  <span class="tag">
                    <a href="https://">Paper</a>&nbsp;
                    <a href="https://github.com/hzyjerry/ood-human">Code</a>&nbsp;
                    <!-- <a href="https://bair.berkeley.edu/blog/2021/11/19/trajectory-transformer/">Blog</a>&nbsp; -->
                    <a href="files/bib.txt">BibTex</a>&nbsp;
                  </span>
                </div>
              </li>
          </div>
        </div>

      </div>
    </div>

<br><br><br><br>


</div></body></html>
